import requests
from bs4 import BeautifulSoup
import csv
import time

BASE_URL = "http://books.toscrape.com/catalogue/page-{}.html"
MAX_RETRIES = 3

def fetch_page(page_number):
    retries = 0
    while retries < MAX_RETRIES:
        try:
            url = BASE_URL.format(page_number)
            response = requests.get(url, timeout=5)
            if response.status_code == 200:
                return response.text
            else:
                print(f"Error {response.status_code}")
                retries += 1
        except Exception as e:
            print(f"Retry {retries+1}/{MAX_RETRIES} - {e}")
            retries += 1
            time.sleep(2)
    return None

def parse_books(html):
    soup = BeautifulSoup(html, 'html.parser')
    books = []
    for book in soup.select('.product_pod'):
        title = book.h3.a['title']
        price = book.select_one('.price_color').text
        stock = book.select_one('.availability').text.strip()
        books.append((title, price, stock))
    return books

def scrape_all_books(pages=5):
    all_books = []
    for i in range(1, pages+1):
        print(f"Scraping page {i}")
        html = fetch_page(i)
        if html:
            books = parse_books(html)
            all_books.extend(books)
        else:
            print(f"Failed to fetch page {i}")
    return all_books

def save_to_csv(data, filename="books.csv"):
    with open(filename, mode='w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(["Title", "Price", "Stock"])
        writer.writerows(data)

if __name__ == "__main__":
    books_data = scrape_all_books(pages=10)
    save_to_csv(books_data)
    print("Scraping complete. Data saved to books.csv.")
